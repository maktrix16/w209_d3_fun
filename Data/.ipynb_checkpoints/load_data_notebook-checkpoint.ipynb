{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_data.txt\n",
    "9,42.97456,119.332025,492,2008-08-20,12:09:04,walk\n",
    "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\n",
    "18,39.974893,136.331959,467,2008-08-20,12:10:35,NA\n",
    "18,39.974903,126.331965,466,2008-08-20,12:10:37,subway\n",
    "18,31.97492,136.331962,466,2008-08-20,12:10:39,subway\n",
    "120,32.974927,116.331959,466,2008-08-20,12:10:41,walk\n",
    "120,40.974928,116.331944,466,2008-08-20,12:10:43,walk\n",
    "120,42.974933,120.33194,465,2008-08-20,12:10:45,bus\n",
    "120,44.974927,116.331928,465,2008-08-20,12:10:47,subway\n",
    "120,45.974927,116.331927,465,2008-08-20,12:10:49,walk\n",
    "9,40.97456,106.332025,492,2008-08-20,12:09:04,walk\n",
    "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\n",
    "18,39.974893,108.331959,467,2008-08-20,12:10:35,bus\n",
    "18,39.974903,121.331965,466,2008-08-20,12:10:37,walk\n",
    "18,39.97492,106.331962,466,2008-08-20,12:10:39,bus\n",
    "120,39.974927,112.331959,466,2008-08-20,12:10:41,walk\n",
    "120,39.974928,122.331944,466,2008-08-20,12:10:43,walk\n",
    "120,42.974933,118.33194,465,2008-08-20,12:10:45,bus\n",
    "120,44.974927,110.331928,465,2008-08-20,12:10:47,walk\n",
    "120,45.974927,100.331927,465,2008-08-20,12:10:49,walk\n",
    "9,41.97456,118.332025,492,2008-08-20,12:09:04,walk\n",
    "9,39.974658,119.332114,492,2008-08-20,12:09:07,bus\n",
    "18,39.974893,136.331959,467,2008-08-20,12:10:35,bus\n",
    "18,39.974903,116.331965,466,2008-08-20,12:10:37,walk\n",
    "18,34.97492,104.331962,466,2008-08-20,12:10:39,bus\n",
    "120,33.974927,116.331959,466,2008-08-20,12:10:41,walk\n",
    "120,38.974928,107.331944,466,2008-08-20,12:10:43,walk\n",
    "120,44.974933,119.33194,465,2008-08-20,12:10:45,bus\n",
    "120,30.974927,120.331928,465,2008-08-20,12:10:47,bus\n",
    "120,25.974927,123.331927,465,2008-08-20,12:10:49,walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,39.97456,116.332025,492,2008-08-20,12:09:04,walk\r\n",
      "9,39.974658,116.332114,492,2008-08-20,12:09:07,bus\r\n",
      "18,39.974893,116.331959,467,2008-08-20,12:10:35,NA\r\n",
      "18,39.974903,116.331965,466,2008-08-20,12:10:37,subway\r\n",
      "18,39.97492,116.331962,466,2008-08-20,12:10:39,subway\r\n",
      "120,39.974927,116.331959,466,2008-08-20,12:10:41,walk\r\n",
      "120,39.974928,116.331944,466,2008-08-20,12:10:43,walk\r\n",
      "120,39.974933,116.33194,465,2008-08-20,12:10:45,bus\r\n",
      "120,39.974927,116.331928,465,2008-08-20,12:10:47,subway\r\n",
      "120,39.974927,116.331927,465,2008-08-20,12:10:49,walk\r\n",
      "9,40.97456,106.332025,492,2008-08-20,12:09:04,walk\r\n",
      "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\r\n",
      "18,39.974893,106.331959,467,2008-08-20,12:10:35,bus\r\n",
      "18,39.974903,106.331965,466,2008-08-20,12:10:37,walk\r\n",
      "18,39.97492,106.331962,466,2008-08-20,12:10:39,bus\r\n",
      "120,39.974927,106.331959,466,2008-08-20,12:10:41,walk\r\n",
      "120,39.974928,106.331944,466,2008-08-20,12:10:43,walk\r\n",
      "120,40.974933,116.33194,465,2008-08-20,12:10:45,bus\r\n",
      "120,40.974927,116.331928,465,2008-08-20,12:10:47,walk\r\n",
      "120,40.974927,116.331927,465,2008-08-20,12:10:49,walk\r\n",
      "9,41.97456,118.332025,492,2008-08-20,12:09:04,walk\r\n",
      "9,39.974658,119.332114,492,2008-08-20,12:09:07,bus\r\n",
      "18,39.974893,136.331959,467,2008-08-20,12:10:35,bus\r\n",
      "18,39.974903,116.331965,466,2008-08-20,12:10:37,walk\r\n",
      "18,34.97492,116.331962,466,2008-08-20,12:10:39,bus\r\n",
      "120,33.974927,116.331959,466,2008-08-20,12:10:41,walk\r\n",
      "120,38.974928,118.331944,466,2008-08-20,12:10:43,walk\r\n",
      "120,39.974933,119.33194,465,2008-08-20,12:10:45,bus\r\n",
      "120,39.974927,120.331928,465,2008-08-20,12:10:47,bus\r\n",
      "120,39.974927,121.331927,465,2008-08-20,12:10:49,walk"
     ]
    }
   ],
   "source": [
    "!cat test_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load.py\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import datetime, pytz\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "# from datetime import datetime\n",
    "# from dateutil import tz\n",
    "\n",
    "class LoadData():\n",
    "\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.w209project #create/set database name as \"w209project\"\n",
    "    collection = db.locs    #create/set collection name as \"locs\"\n",
    "    \n",
    "    def removeData(self):\n",
    "        self.collection.remove({})\n",
    "    \n",
    "    def load(self,data_file):\n",
    "        with open(data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                user_id,latitude,longitude,altitude,date,time,transport = line.strip().split(',')\n",
    "                user_id = int(user_id)\n",
    "                latidude = float(latitude)\n",
    "                longitude = float(longitude)\n",
    "                altidude = float(altitude)\n",
    "\n",
    "                #ensure it's china timezone to ensure correct timezone input\n",
    "                local = pytz.timezone (\"Asia/Shanghai\")\n",
    "                naive = datetime.datetime.strptime (date+\" \"+time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                local_dt = local.localize(naive, is_dst=None)\n",
    "#                 utc_dt = local_dt.astimezone (pytz.utc)\n",
    "#                 print latitude,longitude,altitude,date,time,transport\n",
    "                \n",
    "                location = {\"user_id\":user_id,\n",
    "                            \"latitude\":latitude,\n",
    "                            \"longitude\":longitude,\n",
    "                            \"altitude\":altitude,\n",
    "                            \"date_time\":local_dt,\n",
    "                            \"transport\":transport,\n",
    "                            \"created_at\":datetime.datetime.utcnow()}\n",
    "                self.collection.insert_one(location)\n",
    "\n",
    "            print '\\n',self.collection.count(),'records loaded into MongoDB'\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    l = LoadData()\n",
    "    data_file = sys.argv[1]\n",
    "    remove_data = sys.argv[2]  #if true, then remove existing data\n",
    "    if remove_data:\n",
    "        l.removeData()\n",
    "    l.load(data_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "30 records loaded into MongoDB\r\n"
     ]
    }
   ],
   "source": [
    "!python load.py test_data.txt T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load.py\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.bucket import Bucket\n",
    "from boto.s3.key import Key\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import datetime\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "BUCKET_NAME = 'w205-price-comparison-tool'\n",
    "\n",
    "class LoadData():\n",
    "\n",
    "    db = None\n",
    "    client = None\n",
    "    bucket = None\n",
    "\n",
    "    def connectToDatabase(self):\n",
    "        # connect to database\n",
    "        #client = MongoClient()\n",
    "        self.client = MongoClient('localhost', 27017)\n",
    "        # client = MongoClient('mongodb://localhost:27017/')\n",
    "        self.db = self.client.w205project\n",
    "        # conn = S3Connection('CHANGE_THIS_your_key_id','CHANGE_THIS_your_secret_access_key')\n",
    "        conn = S3Connection()\n",
    "        self.bucket = conn.get_bucket(BUCKET_NAME)\n",
    "\n",
    "    # helper function to convert num to double-digit string (eg. 4 -> \"04\")\n",
    "    def convertNum(self, num_int):\n",
    "        num_string = str(num_int)\n",
    "        if num_int < 10:\n",
    "            num_string = \"0\"+num_string\n",
    "        return num_string\n",
    "\n",
    "    # helper function to convert string to boolean values\n",
    "    def convertBoolean(self, string):\n",
    "        if string == \"T\" or string == \"True\":\n",
    "            return True\n",
    "        if string == \"F\" or string == \"False\":\n",
    "            return False\n",
    "\n",
    "    # main function to load data into MongoDB\n",
    "    def main(self,loc_c,prod_c,list_c,listU_c,mode,load_listing_via_s3,erase_listings,from_date_s=\"\",to_date_s=\"\"):\n",
    "\n",
    "        if mode=='supervised':\n",
    "            CLEAN_KEY_PREFIX = \"clean/supervised/\"\n",
    "            keyname_data_start_index = 17  #used for determining scraped data below\n",
    "            keyname_data_end_index = 27\n",
    "            collectionUniqueListing = 'listing_u_sups'  #used for aggregation function output for unique listing\n",
    "        elif mode == 'unsupervised':\n",
    "            CLEAN_KEY_PREFIX = \"clean/unsupervised/\"\n",
    "            keyname_data_start_index = 19\n",
    "            keyname_data_end_index = 29\n",
    "            collectionUniqueListing = 'listing_u_unsups'\n",
    "        else:\n",
    "            print \"Incorrect mode entered. Please enter 'supervised' or 'unsupervised' only\"\n",
    "            return\n",
    "    \n",
    "        # format date-time (into UTC)\n",
    "        if from_date_s == \"\":\n",
    "            from_date = datetime.strptime('1900-01-01','%Y-%m-%d')\n",
    "        else:\n",
    "            from_date = datetime.strptime(from_date_s,'%Y-%m-%d')\n",
    "    \n",
    "        if to_date_s == \"\":\n",
    "            to_date = datetime.utcnow()\n",
    "        else:\n",
    "            to_date = datetime.strptime(to_date_s,'%Y-%m-%d')\n",
    "\n",
    "\n",
    "        # clear old entries\n",
    "        prod_c.remove({})\n",
    "        loc_c.remove({})\n",
    "        listU_c.remove({})\n",
    "        if erase_listings:\n",
    "            list_c.remove({})\n",
    "    \n",
    "        if load_listing_via_s3:\n",
    "            # get cleaned data key from S3\n",
    "            print \"Reading key name from S3\"\n",
    "            exp = re.compile(CLEAN_KEY_PREFIX)\n",
    "            key_list = []\n",
    "            for key in self.bucket.list():\n",
    "                if exp.match(key.name):\n",
    "                    scrape_date_s = key.name[keyname_data_start_index:keyname_data_end_index]\n",
    "                    scrape_date = datetime.strptime(scrape_date_s, '%Y-%m-%d')\n",
    "                    # print \"from_date: \", from_date\n",
    "                    # print \"scrape_date: \", scrape_date\n",
    "                    # print \"to_date: \", to_date\n",
    "                    # print \"check from_date: \", scrape_date >= from_date\n",
    "                    # print \"check to_date: \", scrape_date <= to_date\n",
    "                    if scrape_date >= from_date and scrape_date <= to_date:\n",
    "                        key_list.append(key)\n",
    "\n",
    "            # loop through each city json file\n",
    "            print \"\\nStarting process of S3 -> MongoDB (\"+mode+\" data)\"\n",
    "            countKey = 0\n",
    "            countListing = 0\n",
    "            listings_mongo = []\n",
    "\n",
    "\n",
    "            for key in key_list:\n",
    "                json_string = key.get_contents_as_string()\n",
    "\n",
    "            # for i in range(0,2):\n",
    "            #     key = key_list[i]\n",
    "            #     json_string = key_list[0].get_contents_as_string()\n",
    "\n",
    "                listings = json.loads(json_string)\n",
    "\n",
    "                #date on c3 chart is the scrape date (not date of posting creation)\n",
    "                scrape_date_s = key.name[keyname_data_start_index:keyname_data_end_index]\n",
    "\n",
    "                # enter each listing per city json file into MongoDB\n",
    "                for listing in listings:\n",
    "                    pred_class = listing['pred_class']\n",
    "                    if pred_class != -1:\n",
    "                        if pred_class == 0:\n",
    "                            product = \"iPhone (unclassified)\"\n",
    "                        else:\n",
    "                            product = \"iPhone \"+pred_class\n",
    "                        # print \"mongo scrape date: \",scrape_date_s\n",
    "                        listing_mongo = {\n",
    "                            \"city\":listing['area'],\n",
    "                            \"product\":product,\n",
    "                            \"price\": float(listing['price']),\n",
    "                            \"title\":listing['title'],\n",
    "                            \"url\":listing['url'].strip('\\\\'),\n",
    "                            \"scraped_at\":datetime.strptime(scrape_date_s,\"%Y-%m-%d\"),\n",
    "                            \"created_at\":datetime.strptime(listing['create_date'][:10],\"%Y-%m-%d\"),\n",
    "                            \"c3Date\":scrape_date_s\n",
    "                        }\n",
    "\n",
    "                        listings_mongo.append(listing_mongo)\n",
    "                        countListing = countListing + 1\n",
    "\n",
    "                        # list_c.insert(listings_mongo) #for testing only\n",
    "                        # listings_mongo = [] #for testing only\n",
    "\n",
    "                # print \"key_list:\" + str(len(key_list))\n",
    "                # print \"countKey:\" + str(countKey)\n",
    "                # print scrape_date_s\n",
    "                # print \"hello: \"+str(listings_mongo)+\" ... end hello\"\n",
    "\n",
    "                if len(listings_mongo)>10000:\n",
    "                    print \"10000 listings are obtained from S3, now inserting into MongoDB\"\n",
    "                    list_c.insert(listings_mongo)\n",
    "                    listings_mongo = []\n",
    "                elif countKey == len(key_list)-1:\n",
    "                    print \"reached the last key in S3, now inserting into MongoDB\"\n",
    "                    list_c.insert(listings_mongo)\n",
    "\n",
    "                countKey = countKey + 1\n",
    "                print \"S3 -> MongDB progress (\"+mode+\"): \"+ str(round(countKey/float(len(key_list))*100,1))+\"%\"\n",
    "    \n",
    "        # insert unique product into unique listing collection in MongoDB, keepying latest post date\n",
    "        pipe = \t[\n",
    "            {\n",
    "                \"$group\" : {\n",
    "                    \"_id\":\"$url\",  # _id is now the url for this unique listing\n",
    "                    \"product\":{\"$first\":\"$product\"},\n",
    "                    \"title\":{\"$first\":\"$title\"},\n",
    "                    \"price\":{\"$first\":\"$price\"},\n",
    "                    \"city\":{\"$first\":\"$city\"},\n",
    "                    \"created_at\":{\"$first\":\"$created_at\"},\n",
    "                    \"url\":{\"$first\":\"$url\"}\n",
    "                }\n",
    "            },\n",
    "            {\"$sort\": {\"created_at\":-1}},\n",
    "            {\"$out\" : collectionUniqueListing}\n",
    "        ]\n",
    "        list_c.aggregate(pipeline=pipe,allowDiskUse=True)\n",
    "\n",
    "        # insert unique product into products collection in MongoDB\n",
    "        print \"\\nFollowing products are being entered into database (\" +mode+\")\"\n",
    "        prod_c.insert({\"product\":\"All iPhones\"})\n",
    "        pipe = \t[\n",
    "            {\n",
    "                \"$group\" : {\n",
    "                    \"_id\":\"$product\",\n",
    "                    \"posts\": {\"$sum\":1}\n",
    "                }\n",
    "            },\n",
    "            {\"$sort\" : {\"_id\":1}}\n",
    "        ]\n",
    "        info_by_product = listU_c.aggregate(pipeline=pipe)\n",
    "        productList = []\n",
    "        for product in info_by_product[\"result\"]:\n",
    "            if product[\"_id\"] not in productList:\n",
    "                productList.append(product[\"_id\"])\n",
    "                prod_c.insert({\"product\":product[\"_id\"]})\n",
    "        print productList\n",
    "\n",
    "        # insert unique location into locations collection in MongoDB\n",
    "        print \"\\nFollowing products are being entered into database (\" +mode+\")\"\n",
    "        loc_c.insert({\"location\":\"Throughout U.S.\"})\n",
    "        pipe = \t[\n",
    "            {\n",
    "                \"$group\" : {\n",
    "                    \"_id\":\"$city\",\n",
    "                    \"posts\": {\"$sum\":1}\n",
    "                }\n",
    "            },\n",
    "            {\"$sort\" : {\"_id\":1}}\n",
    "        ]\n",
    "        info_by_location = listU_c.aggregate(pipeline=pipe)\n",
    "        locationList = []\n",
    "        for location in info_by_location[\"result\"]:\n",
    "            if location[\"_id\"] not in locationList:\n",
    "                locationList.append(location[\"_id\"])\n",
    "                loc_c.insert({\"location\":location[\"_id\"]})\n",
    "        print locationList\n",
    "        print \"\\n##############################################\"\n",
    "\n",
    "    def load(self, mode, new, erase, start, end):\n",
    "\n",
    "        self.connectToDatabase()\n",
    "\n",
    "        input_s3_listing = self.convertBoolean(new)\n",
    "        input_erase_listing = self.convertBoolean(erase)\n",
    "\n",
    "        if mode ==\"supervised\" or mode ==\"both\":\n",
    "            self.main(self.db.loc_sups,self.db.prod_sups,self.db.listing_sups,self.db.listing_u_sups,\n",
    "                \"supervised\",input_s3_listing,input_erase_listing,start,end)\n",
    "        if mode ==\"unsupervised\" or mode ==\"both\":\n",
    "            self.main(self.db.loc_unsups,self.db.prod_unsups,self.db.listing_unsups,self.db.listing_u_unsups,\n",
    "                \"unsupervised\",input_s3_listing,input_erase_listing,start,end)\n",
    "        \n",
    "        print '\\nSummary of data loaded in MongoDB for SUPERVISED mode:'\n",
    "        print 'No. of products:         '+str(self.db.prod_sups.count())\n",
    "        print 'No. of locations:        '+str(self.db.loc_sups.count())\n",
    "        print 'No. of general listings: '+str(self.db.listing_sups.count())\n",
    "        print 'No. of unique listings:  '+str(self.db.listing_u_sups.count())\n",
    "    \n",
    "        print '\\nSummary of data loaded in MongoDB for UNSUPERVISED mode:'\n",
    "        print 'No. of products:         '+str(self.db.prod_unsups.count())\n",
    "        print 'No. of locations:        '+str(self.db.loc_unsups.count())\n",
    "        print 'No. of general listings: '+str(self.db.listing_unsups.count())\n",
    "        print 'No. of unique listings:  '+str(self.db.listing_u_unsups.count())+'\\n'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Input without providing external argument\n",
    "    # Command line sample:   python load.py\n",
    "    # main(db.loc_unsups,db.prod_unsups,db.listing_unsups,db.listing_u_unsups,'unsupervised',True,True)\n",
    "    # main(db.loc_sups,db.prod_sups,db.listing_sups,db.listing_u_sups,'supervised',True,True,'2015-04-15','2015-04-15')\n",
    "    \n",
    "    # Input without providing external argument\n",
    "    # Command line sample:   python load.py 'supervised' True True '2015-04-15' '2015-04-17'\n",
    "    # Input via running command line script\n",
    "    l = LoadData()\n",
    "    l.load(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
