{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Notebook for creating test data and data-loading script for MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_data.txt\n",
    "9,42.97456,119.332025,492,2008-08-20,12:09:04,walk\n",
    "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\n",
    "18,39.974893,136.331959,467,2008-08-20,12:10:35,NA\n",
    "18,39.974903,126.331965,466,2008-08-20,12:10:37,subway\n",
    "18,31.97492,136.331962,466,2008-08-20,12:10:39,subway\n",
    "120,32.974927,116.331959,466,2008-08-20,12:10:41,walk\n",
    "120,40.974928,116.331944,466,2008-08-20,12:10:43,walk\n",
    "120,42.974933,120.33194,465,2008-08-20,12:10:45,bus\n",
    "120,44.974927,116.331928,465,2008-08-20,12:10:47,subway\n",
    "120,45.974927,116.331927,465,2008-08-20,12:10:49,walk\n",
    "9,40.97456,106.332025,492,2008-08-20,12:09:04,walk\n",
    "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\n",
    "18,39.974893,108.331959,467,2008-08-20,12:10:35,bus\n",
    "18,39.974903,121.331965,466,2008-08-20,12:10:37,walk\n",
    "18,39.97492,106.331962,466,2008-08-20,12:10:39,bus\n",
    "120,39.974927,112.331959,466,2008-08-20,12:10:41,walk\n",
    "120,39.974928,122.331944,466,2008-08-20,12:10:43,walk\n",
    "120,42.974933,118.33194,465,2008-08-20,12:10:45,bus\n",
    "120,44.974927,110.331928,465,2008-08-20,12:10:47,walk\n",
    "120,45.974927,100.331927,465,2008-08-20,12:10:49,walk\n",
    "9,41.97456,118.332025,492,2008-08-20,12:09:04,walk\n",
    "9,39.974658,119.332114,492,2008-08-20,12:09:07,bus\n",
    "18,39.974893,136.331959,467,2008-08-20,12:10:35,bus\n",
    "18,39.974903,116.331965,466,2008-08-20,12:10:37,walk\n",
    "18,34.97492,104.331962,466,2008-08-20,12:10:39,bus\n",
    "120,33.974927,116.331959,466,2008-08-20,12:10:41,walk\n",
    "120,38.974928,107.331944,466,2008-08-20,12:10:43,walk\n",
    "120,44.974933,119.33194,465,2008-08-20,12:10:45,bus\n",
    "120,30.974927,120.331928,465,2008-08-20,12:10:47,bus\n",
    "120,25.974927,123.331927,465,2008-08-20,12:10:49,walk\n",
    "121,25.974927,123.331927,465,2008-08-20,12:10:49,walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,42.97456,119.332025,492,2008-08-20,12:09:04,walk\r\n",
      "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\r\n",
      "18,39.974893,136.331959,467,2008-08-20,12:10:35,NA\r\n",
      "18,39.974903,126.331965,466,2008-08-20,12:10:37,subway\r\n",
      "18,31.97492,136.331962,466,2008-08-20,12:10:39,subway\r\n",
      "120,32.974927,116.331959,466,2008-08-20,12:10:41,walk\r\n",
      "120,40.974928,116.331944,466,2008-08-20,12:10:43,walk\r\n",
      "120,42.974933,120.33194,465,2008-08-20,12:10:45,bus\r\n",
      "120,44.974927,116.331928,465,2008-08-20,12:10:47,subway\r\n",
      "120,45.974927,116.331927,465,2008-08-20,12:10:49,walk\r\n",
      "9,40.97456,106.332025,492,2008-08-20,12:09:04,walk\r\n",
      "9,39.974658,106.332114,492,2008-08-20,12:09:07,bus\r\n",
      "18,39.974893,108.331959,467,2008-08-20,12:10:35,bus\r\n",
      "18,39.974903,121.331965,466,2008-08-20,12:10:37,walk\r\n",
      "18,39.97492,106.331962,466,2008-08-20,12:10:39,bus\r\n",
      "120,39.974927,112.331959,466,2008-08-20,12:10:41,walk\r\n",
      "120,39.974928,122.331944,466,2008-08-20,12:10:43,walk\r\n",
      "120,42.974933,118.33194,465,2008-08-20,12:10:45,bus\r\n",
      "120,44.974927,110.331928,465,2008-08-20,12:10:47,walk\r\n",
      "120,45.974927,100.331927,465,2008-08-20,12:10:49,walk\r\n",
      "9,41.97456,118.332025,492,2008-08-20,12:09:04,walk\r\n",
      "9,39.974658,119.332114,492,2008-08-20,12:09:07,bus\r\n",
      "18,39.974893,136.331959,467,2008-08-20,12:10:35,bus\r\n",
      "18,39.974903,116.331965,466,2008-08-20,12:10:37,walk\r\n",
      "18,34.97492,104.331962,466,2008-08-20,12:10:39,bus\r\n",
      "120,33.974927,116.331959,466,2008-08-20,12:10:41,walk\r\n",
      "120,38.974928,107.331944,466,2008-08-20,12:10:43,walk\r\n",
      "120,44.974933,119.33194,465,2008-08-20,12:10:45,bus\r\n",
      "120,30.974927,120.331928,465,2008-08-20,12:10:47,bus\r\n",
      "120,25.974927,123.331927,465,2008-08-20,12:10:49,walk"
     ]
    }
   ],
   "source": [
    "!cat test_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load.py\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import datetime, pytz\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "# from datetime import datetime\n",
    "# from dateutil import tz\n",
    "\n",
    "class LoadData():\n",
    "\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.w209project #create/set database name as \"w209project\"\n",
    "    collection = db.locs    #create/set collection name as \"locs\"\n",
    "    \n",
    "    def removeData(self):\n",
    "        self.collection.remove({})\n",
    "    \n",
    "    def load(self,data_file, num_data):\n",
    "        #set limit on number of lines of data\n",
    "        if num_data==\"all\": limit=1e15\n",
    "        else: limit=int(num_data)\n",
    "        count=0\n",
    "\n",
    "        with open(data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if count>=limit: break\n",
    "                \n",
    "                user_id,latitude,longitude,altitude,date,time,transport = line.strip().split(',')\n",
    "                user_id = int(user_id)\n",
    "                latidude = float(latitude)\n",
    "                longitude = float(longitude)\n",
    "                altidude = float(altitude)\n",
    "\n",
    "                #ensure it's china timezone to ensure correct timezone input\n",
    "                local = pytz.timezone (\"Asia/Shanghai\")\n",
    "                naive = datetime.datetime.strptime (date+\" \"+time, \"%Y-%m-%d %H:%M:%S\")\n",
    "                local_dt = local.localize(naive, is_dst=None)\n",
    "#                 utc_dt = local_dt.astimezone (pytz.utc)\n",
    "#                 print latitude,longitude,altitude,date,time,transport\n",
    "                \n",
    "                location = {\"user_id\":user_id,\n",
    "                            \"latitude\":latitude,\n",
    "                            \"longitude\":longitude,\n",
    "                            \"altitude\":altitude,\n",
    "                            \"date_time\":local_dt,\n",
    "                            \"transport\":transport,\n",
    "                            \"created_at\":datetime.datetime.utcnow()}\n",
    "                self.collection.insert_one(location)\n",
    "            \n",
    "                count+=1\n",
    "\n",
    "            print '\\n',self.collection.count(),'records loaded into MongoDB'\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    l = LoadData()\n",
    "    data_file = sys.argv[1]    #name of data file\n",
    "    remove_data = sys.argv[2]  #if true, then remove existing data\n",
    "    num_data = sys.argv[3]     #number of data points (ie. no. of lines of data)\n",
    "\n",
    "    if remove_data==\"T\":\n",
    "        l.removeData()\n",
    "    l.load(data_file,num_data.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install intervaltree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting importdata.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile importdata.sh\n",
    "#!/bin/bash\n",
    "FILES=\"../../Geolife Trajectories 1.3/data_csv\"\n",
    "for f in \"$FILES\"/*.csv\n",
    "do\n",
    "  echo \"Processing ${f} data file...\"\n",
    "  python load.py \"${f}\" F all\n",
    "done\n",
    "# http://www.cyberciti.biz/faq/bash-loop-over-file/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../../Geolife Trajectories 1.3/data_csv/000.csv data file...\n",
      "\n",
      "173870 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/001.csv data file...\n",
      "\n",
      "282477 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/002.csv data file...\n",
      "\n",
      "530694 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/003.csv data file...\n",
      "\n",
      "1015920 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/004.csv data file...\n",
      "\n",
      "1455317 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/005.csv data file...\n",
      "\n",
      "1564363 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/006.csv data file...\n",
      "\n",
      "1596193 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/007.csv data file...\n",
      "\n",
      "1683410 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/008.csv data file...\n",
      "\n",
      "1761320 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/009.csv data file...\n",
      "\n",
      "1845936 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/010.csv data file...\n",
      "\n",
      "2781512 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/011.csv data file...\n",
      "\n",
      "2872315 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/012.csv data file...\n",
      "\n",
      "3019407 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/013.csv data file...\n",
      "\n",
      "3310589 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/014.csv data file...\n",
      "\n",
      "3698802 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/015.csv data file...\n",
      "\n",
      "3786538 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/016.csv data file...\n",
      "\n",
      "3875810 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/017.csv data file...\n",
      "\n",
      "4901989 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/018.csv data file...\n",
      "\n",
      "4949268 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/019.csv data file...\n",
      "\n",
      "4997092 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/020.csv data file...\n",
      "\n",
      "5174773 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/021.csv data file...\n",
      "\n",
      "5177158 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/022.csv data file...\n",
      "\n",
      "5648595 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/023.csv data file...\n",
      "\n",
      "5768945 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/024.csv data file...\n",
      "\n",
      "6032427 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/025.csv data file...\n",
      "\n",
      "6662323 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/026.csv data file...\n",
      "\n",
      "6810734 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/027.csv data file...\n",
      "\n",
      "6826242 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/028.csv data file...\n",
      "\n",
      "6950070 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/029.csv data file...\n",
      "\n",
      "7033914 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/030.csv data file...\n",
      "\n",
      "7649862 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/031.csv data file...\n",
      "\n",
      "7670274 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/032.csv data file...\n",
      "\n",
      "7696742 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/033.csv data file...\n",
      "\n",
      "7766716 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/034.csv data file...\n",
      "\n",
      "7933386 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/035.csv data file...\n",
      "\n",
      "8245428 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/036.csv data file...\n",
      "\n",
      "8497401 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/037.csv data file...\n",
      "\n",
      "8688725 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/038.csv data file...\n",
      "\n",
      "8939118 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/039.csv data file...\n",
      "\n",
      "9206855 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/040.csv data file...\n",
      "\n",
      "9262868 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/041.csv data file...\n",
      "\n",
      "10319911 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/042.csv data file...\n",
      "\n",
      "10559359 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/043.csv data file...\n",
      "\n",
      "10655605 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/044.csv data file...\n",
      "\n",
      "10732451 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/045.csv data file...\n",
      "\n",
      "10742194 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/046.csv data file...\n",
      "\n",
      "10764012 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/047.csv data file...\n",
      "\n",
      "10764802 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/048.csv data file...\n",
      "\n",
      "10773878 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/049.csv data file...\n",
      "\n",
      "10776492 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/050.csv data file...\n",
      "\n",
      "11113351 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/051.csv data file...\n",
      "\n",
      "11130370 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/052.csv data file...\n",
      "\n",
      "11503415 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/053.csv data file...\n",
      "\n",
      "11503751 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/054.csv data file...\n",
      "\n",
      "11508531 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/055.csv data file...\n",
      "\n",
      "11546317 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/056.csv data file...\n",
      "\n",
      "11550321 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/057.csv data file...\n",
      "\n",
      "11553115 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/058.csv data file...\n",
      "\n",
      "11577884 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/059.csv data file...\n",
      "\n",
      "11601490 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/060.csv data file...\n",
      "\n",
      "11601507 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/061.csv data file...\n",
      "\n",
      "11604307 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/062.csv data file...\n",
      "\n",
      "11934609 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/063.csv data file...\n",
      "\n",
      "11947099 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/064.csv data file...\n",
      "\n",
      "12005853 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/065.csv data file...\n",
      "\n",
      "12431954 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/066.csv data file...\n",
      "\n",
      "12492974 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/067.csv data file...\n",
      "\n",
      "12887303 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/068.csv data file...\n",
      "\n",
      "13825179 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/069.csv data file...\n",
      "\n",
      "13841648 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/070.csv data file...\n",
      "\n",
      "13876959 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/071.csv data file...\n",
      "\n",
      "14000809 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/072.csv data file...\n",
      "\n",
      "14000890 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/073.csv data file...\n",
      "\n",
      "14044025 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/074.csv data file...\n",
      "\n",
      "14219723 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/075.csv data file...\n",
      "\n",
      "14257272 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/076.csv data file...\n",
      "\n",
      "14258233 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/077.csv data file...\n",
      "\n",
      "14261377 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/078.csv data file...\n",
      "\n",
      "14336886 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/079.csv data file...\n",
      "\n",
      "14348129 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/080.csv data file...\n",
      "\n",
      "14348840 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/081.csv data file...\n",
      "\n",
      "14393092 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/082.csv data file...\n",
      "\n",
      "14565639 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/083.csv data file...\n",
      "\n",
      "14600691 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/084.csv data file...\n",
      "\n",
      "15002023 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/085.csv data file...\n",
      "\n",
      "15603894 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/086.csv data file...\n",
      "\n",
      "15604455 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/087.csv data file...\n",
      "\n",
      "15604607 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/088.csv data file...\n",
      "\n",
      "15631637 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/089.csv data file...\n",
      "\n",
      "15662559 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/090.csv data file...\n",
      "\n",
      "15662879 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/091.csv data file...\n",
      "\n",
      "15674305 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/092.csv data file...\n",
      "\n",
      "15729965 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/093.csv data file...\n",
      "\n",
      "15738863 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/094.csv data file...\n",
      "\n",
      "15741657 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/095.csv data file...\n",
      "\n",
      "15787216 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/096.csv data file...\n",
      "\n",
      "16018304 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/097.csv data file...\n",
      "\n",
      "16019858 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/098.csv data file...\n",
      "\n",
      "16020671 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/099.csv data file...\n",
      "\n",
      "16021938 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/100.csv data file...\n",
      "\n",
      "16028195 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/101.csv data file...\n",
      "\n",
      "16041933 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/102.csv data file...\n",
      "\n",
      "16048611 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/103.csv data file...\n",
      "\n",
      "16085187 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/104.csv data file...\n",
      "\n",
      "16123759 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/105.csv data file...\n",
      "\n",
      "16125736 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/106.csv data file...\n",
      "\n",
      "16127721 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/107.csv data file...\n",
      "\n",
      "16127920 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/108.csv data file...\n",
      "\n",
      "16128404 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/109.csv data file...\n",
      "\n",
      "16128999 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/110.csv data file...\n",
      "\n",
      "16134519 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/111.csv data file...\n",
      "\n",
      "16149845 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/112.csv data file...\n",
      "\n",
      "16240410 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/113.csv data file...\n",
      "\n",
      "16264941 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/114.csv data file...\n",
      "\n",
      "16277273 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/115.csv data file...\n",
      "\n",
      "16425684 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/116.csv data file...\n",
      "\n",
      "16428198 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/117.csv data file...\n",
      "\n",
      "16428539 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/118.csv data file...\n",
      "\n",
      "16429899 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/119.csv data file...\n",
      "\n",
      "16533633 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/120.csv data file...\n",
      "\n",
      "16544423 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/121.csv data file...\n",
      "\n",
      "16549790 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/122.csv data file...\n",
      "\n",
      "16621905 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/123.csv data file...\n",
      "\n",
      "16630733 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/124.csv data file...\n",
      "\n",
      "16750846 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/125.csv data file...\n",
      "\n",
      "16847368 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/126.csv data file...\n",
      "\n",
      "17268937 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/127.csv data file...\n",
      "\n",
      "17291790 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/128.csv data file...\n",
      "\n",
      "18500290 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/129.csv data file...\n",
      "\n",
      "18516628 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/130.csv data file...\n",
      "\n",
      "18559268 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/131.csv data file...\n",
      "\n",
      "18616855 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/132.csv data file...\n",
      "\n",
      "18639833 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/133.csv data file...\n",
      "\n",
      "18647089 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/134.csv data file...\n",
      "\n",
      "18651247 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/135.csv data file...\n",
      "\n",
      "18735671 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/136.csv data file...\n",
      "\n",
      "18736977 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/137.csv data file...\n",
      "\n",
      "18751222 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/138.csv data file...\n",
      "\n",
      "18753573 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/139.csv data file...\n",
      "\n",
      "18754926 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/140.csv data file...\n",
      "\n",
      "19097427 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/141.csv data file...\n",
      "\n",
      "19411561 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/142.csv data file...\n",
      "\n",
      "19589828 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/143.csv data file...\n",
      "\n",
      "19592849 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/144.csv data file...\n",
      "\n",
      "20175698 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/145.csv data file...\n",
      "\n",
      "20185605 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/146.csv data file...\n",
      "\n",
      "20185877 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/147.csv data file...\n",
      "\n",
      "20219768 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/148.csv data file...\n",
      "\n",
      "20261598 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/149.csv data file...\n",
      "\n",
      "20270046 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/150.csv data file...\n",
      "\n",
      "20272840 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/151.csv data file...\n",
      "\n",
      "20274162 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/152.csv data file...\n",
      "\n",
      "20283946 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/153.csv data file...\n",
      "\n",
      "22440940 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/154.csv data file...\n",
      "\n",
      "22443868 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/155.csv data file...\n",
      "\n",
      "22485459 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/156.csv data file...\n",
      "\n",
      "22501736 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/157.csv data file...\n",
      "\n",
      "22504236 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/158.csv data file...\n",
      "\n",
      "22510246 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/159.csv data file...\n",
      "\n",
      "22548990 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/160.csv data file...\n",
      "\n",
      "22577228 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/161.csv data file...\n",
      "\n",
      "22578027 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/162.csv data file...\n",
      "\n",
      "22578366 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/163.csv data file...\n",
      "\n",
      "23547466 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/164.csv data file...\n",
      "\n",
      "23547752 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/165.csv data file...\n",
      "\n",
      "23678642 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/166.csv data file...\n",
      "\n",
      "23680002 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/167.csv data file...\n",
      "\n",
      "24298651 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/168.csv data file...\n",
      "\n",
      "24551673 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/169.csv data file...\n",
      "\n",
      "24599994 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/170.csv data file...\n",
      "\n",
      "24604371 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/171.csv data file...\n",
      "\n",
      "24604548 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/172.csv data file...\n",
      "\n",
      "24647755 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/173.csv data file...\n",
      "\n",
      "24648725 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/174.csv data file...\n",
      "\n",
      "24656030 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/175.csv data file...\n",
      "\n",
      "24656338 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/176.csv data file...\n",
      "\n",
      "24656690 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/177.csv data file...\n",
      "\n",
      "24659645 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/178.csv data file...\n",
      "\n",
      "24659729 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/179.csv data file...\n",
      "\n",
      "24829125 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/180.csv data file...\n",
      "\n",
      "24876291 records loaded into MongoDB\n",
      "Processing ../../Geolife Trajectories 1.3/data_csv/181.csv data file...\n",
      "\n",
      "24876978 records loaded into MongoDB\n"
     ]
    }
   ],
   "source": [
    "!sh importdata.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "31 records loaded into MongoDB\r\n"
     ]
    }
   ],
   "source": [
    "!python load.py test_data.txt T all\n",
    "\n",
    "\n",
    "\n",
    "!python load.py ../../Geolife Trajectories 1.3/output 000.csv F all\n",
    "!python load.py 001.csv F all\n",
    "!python load.py 000.csv F all\n",
    "!python load.py 000.csv F all\n",
    "!python load.py 000.csv F all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PLEASE IGNORE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load.py\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.bucket import Bucket\n",
    "from boto.s3.key import Key\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pymongo\n",
    "import datetime\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "BUCKET_NAME = 'w205-price-comparison-tool'\n",
    "\n",
    "class LoadData():\n",
    "\n",
    "    db = None\n",
    "    client = None\n",
    "    bucket = None\n",
    "\n",
    "    def connectToDatabase(self):\n",
    "        # connect to database\n",
    "        #client = MongoClient()\n",
    "        self.client = MongoClient('localhost', 27017)\n",
    "        # client = MongoClient('mongodb://localhost:27017/')\n",
    "        self.db = self.client.w205project\n",
    "        # conn = S3Connection('CHANGE_THIS_your_key_id','CHANGE_THIS_your_secret_access_key')\n",
    "        conn = S3Connection()\n",
    "        self.bucket = conn.get_bucket(BUCKET_NAME)\n",
    "\n",
    "    # helper function to convert num to double-digit string (eg. 4 -> \"04\")\n",
    "    def convertNum(self, num_int):\n",
    "        num_string = str(num_int)\n",
    "        if num_int < 10:\n",
    "            num_string = \"0\"+num_string\n",
    "        return num_string\n",
    "\n",
    "    # helper function to convert string to boolean values\n",
    "    def convertBoolean(self, string):\n",
    "        if string == \"T\" or string == \"True\":\n",
    "            return True\n",
    "        if string == \"F\" or string == \"False\":\n",
    "            return False\n",
    "\n",
    "    # main function to load data into MongoDB\n",
    "    def main(self,loc_c,prod_c,list_c,listU_c,mode,load_listing_via_s3,erase_listings,from_date_s=\"\",to_date_s=\"\"):\n",
    "\n",
    "        if mode=='supervised':\n",
    "            CLEAN_KEY_PREFIX = \"clean/supervised/\"\n",
    "            keyname_data_start_index = 17  #used for determining scraped data below\n",
    "            keyname_data_end_index = 27\n",
    "            collectionUniqueListing = 'listing_u_sups'  #used for aggregation function output for unique listing\n",
    "        elif mode == 'unsupervised':\n",
    "            CLEAN_KEY_PREFIX = \"clean/unsupervised/\"\n",
    "            keyname_data_start_index = 19\n",
    "            keyname_data_end_index = 29\n",
    "            collectionUniqueListing = 'listing_u_unsups'\n",
    "        else:\n",
    "            print \"Incorrect mode entered. Please enter 'supervised' or 'unsupervised' only\"\n",
    "            return\n",
    "    \n",
    "        # format date-time (into UTC)\n",
    "        if from_date_s == \"\":\n",
    "            from_date = datetime.strptime('1900-01-01','%Y-%m-%d')\n",
    "        else:\n",
    "            from_date = datetime.strptime(from_date_s,'%Y-%m-%d')\n",
    "    \n",
    "        if to_date_s == \"\":\n",
    "            to_date = datetime.utcnow()\n",
    "        else:\n",
    "            to_date = datetime.strptime(to_date_s,'%Y-%m-%d')\n",
    "\n",
    "\n",
    "        # clear old entries\n",
    "        prod_c.remove({})\n",
    "        loc_c.remove({})\n",
    "        listU_c.remove({})\n",
    "        if erase_listings:\n",
    "            list_c.remove({})\n",
    "    \n",
    "        if load_listing_via_s3:\n",
    "            # get cleaned data key from S3\n",
    "            print \"Reading key name from S3\"\n",
    "            exp = re.compile(CLEAN_KEY_PREFIX)\n",
    "            key_list = []\n",
    "            for key in self.bucket.list():\n",
    "                if exp.match(key.name):\n",
    "                    scrape_date_s = key.name[keyname_data_start_index:keyname_data_end_index]\n",
    "                    scrape_date = datetime.strptime(scrape_date_s, '%Y-%m-%d')\n",
    "                    # print \"from_date: \", from_date\n",
    "                    # print \"scrape_date: \", scrape_date\n",
    "                    # print \"to_date: \", to_date\n",
    "                    # print \"check from_date: \", scrape_date >= from_date\n",
    "                    # print \"check to_date: \", scrape_date <= to_date\n",
    "                    if scrape_date >= from_date and scrape_date <= to_date:\n",
    "                        key_list.append(key)\n",
    "\n",
    "            # loop through each city json file\n",
    "            print \"\\nStarting process of S3 -> MongoDB (\"+mode+\" data)\"\n",
    "            countKey = 0\n",
    "            countListing = 0\n",
    "            listings_mongo = []\n",
    "\n",
    "\n",
    "            for key in key_list:\n",
    "                json_string = key.get_contents_as_string()\n",
    "\n",
    "            # for i in range(0,2):\n",
    "            #     key = key_list[i]\n",
    "            #     json_string = key_list[0].get_contents_as_string()\n",
    "\n",
    "                listings = json.loads(json_string)\n",
    "\n",
    "                #date on c3 chart is the scrape date (not date of posting creation)\n",
    "                scrape_date_s = key.name[keyname_data_start_index:keyname_data_end_index]\n",
    "\n",
    "                # enter each listing per city json file into MongoDB\n",
    "                for listing in listings:\n",
    "                    pred_class = listing['pred_class']\n",
    "                    if pred_class != -1:\n",
    "                        if pred_class == 0:\n",
    "                            product = \"iPhone (unclassified)\"\n",
    "                        else:\n",
    "                            product = \"iPhone \"+pred_class\n",
    "                        # print \"mongo scrape date: \",scrape_date_s\n",
    "                        listing_mongo = {\n",
    "                            \"city\":listing['area'],\n",
    "                            \"product\":product,\n",
    "                            \"price\": float(listing['price']),\n",
    "                            \"title\":listing['title'],\n",
    "                            \"url\":listing['url'].strip('\\\\'),\n",
    "                            \"scraped_at\":datetime.strptime(scrape_date_s,\"%Y-%m-%d\"),\n",
    "                            \"created_at\":datetime.strptime(listing['create_date'][:10],\"%Y-%m-%d\"),\n",
    "                            \"c3Date\":scrape_date_s\n",
    "                        }\n",
    "\n",
    "                        listings_mongo.append(listing_mongo)\n",
    "                        countListing = countListing + 1\n",
    "\n",
    "                        # list_c.insert(listings_mongo) #for testing only\n",
    "                        # listings_mongo = [] #for testing only\n",
    "\n",
    "                # print \"key_list:\" + str(len(key_list))\n",
    "                # print \"countKey:\" + str(countKey)\n",
    "                # print scrape_date_s\n",
    "                # print \"hello: \"+str(listings_mongo)+\" ... end hello\"\n",
    "\n",
    "                if len(listings_mongo)>10000:\n",
    "                    print \"10000 listings are obtained from S3, now inserting into MongoDB\"\n",
    "                    list_c.insert(listings_mongo)\n",
    "                    listings_mongo = []\n",
    "                elif countKey == len(key_list)-1:\n",
    "                    print \"reached the last key in S3, now inserting into MongoDB\"\n",
    "                    list_c.insert(listings_mongo)\n",
    "\n",
    "                countKey = countKey + 1\n",
    "                print \"S3 -> MongDB progress (\"+mode+\"): \"+ str(round(countKey/float(len(key_list))*100,1))+\"%\"\n",
    "    \n",
    "        # insert unique product into unique listing collection in MongoDB, keepying latest post date\n",
    "        pipe = \t[\n",
    "            {\n",
    "                \"$group\" : {\n",
    "                    \"_id\":\"$url\",  # _id is now the url for this unique listing\n",
    "                    \"product\":{\"$first\":\"$product\"},\n",
    "                    \"title\":{\"$first\":\"$title\"},\n",
    "                    \"price\":{\"$first\":\"$price\"},\n",
    "                    \"city\":{\"$first\":\"$city\"},\n",
    "                    \"created_at\":{\"$first\":\"$created_at\"},\n",
    "                    \"url\":{\"$first\":\"$url\"}\n",
    "                }\n",
    "            },\n",
    "            {\"$sort\": {\"created_at\":-1}},\n",
    "            {\"$out\" : collectionUniqueListing}\n",
    "        ]\n",
    "        list_c.aggregate(pipeline=pipe,allowDiskUse=True)\n",
    "\n",
    "        # insert unique product into products collection in MongoDB\n",
    "        print \"\\nFollowing products are being entered into database (\" +mode+\")\"\n",
    "        prod_c.insert({\"product\":\"All iPhones\"})\n",
    "        pipe = \t[\n",
    "            {\n",
    "                \"$group\" : {\n",
    "                    \"_id\":\"$product\",\n",
    "                    \"posts\": {\"$sum\":1}\n",
    "                }\n",
    "            },\n",
    "            {\"$sort\" : {\"_id\":1}}\n",
    "        ]\n",
    "        info_by_product = listU_c.aggregate(pipeline=pipe)\n",
    "        productList = []\n",
    "        for product in info_by_product[\"result\"]:\n",
    "            if product[\"_id\"] not in productList:\n",
    "                productList.append(product[\"_id\"])\n",
    "                prod_c.insert({\"product\":product[\"_id\"]})\n",
    "        print productList\n",
    "\n",
    "        # insert unique location into locations collection in MongoDB\n",
    "        print \"\\nFollowing products are being entered into database (\" +mode+\")\"\n",
    "        loc_c.insert({\"location\":\"Throughout U.S.\"})\n",
    "        pipe = \t[\n",
    "            {\n",
    "                \"$group\" : {\n",
    "                    \"_id\":\"$city\",\n",
    "                    \"posts\": {\"$sum\":1}\n",
    "                }\n",
    "            },\n",
    "            {\"$sort\" : {\"_id\":1}}\n",
    "        ]\n",
    "        info_by_location = listU_c.aggregate(pipeline=pipe)\n",
    "        locationList = []\n",
    "        for location in info_by_location[\"result\"]:\n",
    "            if location[\"_id\"] not in locationList:\n",
    "                locationList.append(location[\"_id\"])\n",
    "                loc_c.insert({\"location\":location[\"_id\"]})\n",
    "        print locationList\n",
    "        print \"\\n##############################################\"\n",
    "\n",
    "    def load(self, mode, new, erase, start, end):\n",
    "\n",
    "        self.connectToDatabase()\n",
    "\n",
    "        input_s3_listing = self.convertBoolean(new)\n",
    "        input_erase_listing = self.convertBoolean(erase)\n",
    "\n",
    "        if mode ==\"supervised\" or mode ==\"both\":\n",
    "            self.main(self.db.loc_sups,self.db.prod_sups,self.db.listing_sups,self.db.listing_u_sups,\n",
    "                \"supervised\",input_s3_listing,input_erase_listing,start,end)\n",
    "        if mode ==\"unsupervised\" or mode ==\"both\":\n",
    "            self.main(self.db.loc_unsups,self.db.prod_unsups,self.db.listing_unsups,self.db.listing_u_unsups,\n",
    "                \"unsupervised\",input_s3_listing,input_erase_listing,start,end)\n",
    "        \n",
    "        print '\\nSummary of data loaded in MongoDB for SUPERVISED mode:'\n",
    "        print 'No. of products:         '+str(self.db.prod_sups.count())\n",
    "        print 'No. of locations:        '+str(self.db.loc_sups.count())\n",
    "        print 'No. of general listings: '+str(self.db.listing_sups.count())\n",
    "        print 'No. of unique listings:  '+str(self.db.listing_u_sups.count())\n",
    "    \n",
    "        print '\\nSummary of data loaded in MongoDB for UNSUPERVISED mode:'\n",
    "        print 'No. of products:         '+str(self.db.prod_unsups.count())\n",
    "        print 'No. of locations:        '+str(self.db.loc_unsups.count())\n",
    "        print 'No. of general listings: '+str(self.db.listing_unsups.count())\n",
    "        print 'No. of unique listings:  '+str(self.db.listing_u_unsups.count())+'\\n'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Input without providing external argument\n",
    "    # Command line sample:   python load.py\n",
    "    # main(db.loc_unsups,db.prod_unsups,db.listing_unsups,db.listing_u_unsups,'unsupervised',True,True)\n",
    "    # main(db.loc_sups,db.prod_sups,db.listing_sups,db.listing_u_sups,'supervised',True,True,'2015-04-15','2015-04-15')\n",
    "    \n",
    "    # Input without providing external argument\n",
    "    # Command line sample:   python load.py 'supervised' True True '2015-04-15' '2015-04-17'\n",
    "    # Input via running command line script\n",
    "    l = LoadData()\n",
    "    l.load(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
